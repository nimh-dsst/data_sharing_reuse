{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "from urlextract import URLExtract\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.features.build_features import syns, sep_urls, check_paren, repo_label\n",
    "from src.data.make_dataset import return_passages, test_suitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm doing the following:\n",
    "\n",
    "1. Split data into train and test set.\n",
    "2. Train high recall clf on training and apply to test set.\n",
    "3. Filter the test set to just those predicted to be data statements by the high recall classifier.\n",
    "4. Split that set into train and test.\n",
    "5. Train a high precision classifier on the sub training set.\n",
    "6. Predict on final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_kw(text):\n",
    "    passage_marked = 0\n",
    "    \n",
    "    reg_matches = re.compile(r\"\"\"(software)|(tool)|(code)|(package)|(\\sR\\s)|(python)|\n",
    "                                 (matlab)|(SPM8)|(implement.)\"\"\", re.X|re.VERBOSE)\n",
    "    \n",
    "    m = re.search(reg_matches, text.lower())\n",
    "    if m:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = pd.read_csv('/data/riddleta/data_sharing_reuse/interim/high_recall_labelling - high_recall_labelling.csv')\n",
    "df_labeled['data_statement'] = df_labeled.n2.replace({'c':0, 'n':0, '2':1, 'd':1, 'n2':0, 'nd':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3033, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract = URLExtract()\n",
    "df = pd.read_csv('/data/riddleta/data_sharing_reuse/external/combined_labels_incomplete.csv')\n",
    "df = pd.concat([df[['text', 'section', 'doi', 'Journal Title', \n",
    "                   'pmcid', 'data_statement']],\n",
    "              df_labeled[['text', 'section', 'doi', 'Journal Title', \n",
    "                          'pmcid', 'data_statement']]])\n",
    "df.text.fillna('', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>doi</th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>data_statement</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Golgi-impregnation experiment we found ...</td>\n",
       "      <td>DISCUSS</td>\n",
       "      <td>10.1007/s11481-008-9113-7</td>\n",
       "      <td>J Neuroimmune Pharmacol</td>\n",
       "      <td>2581635</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The individuals with at least 3 usable voxels ...</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>10.1016/j.biopsych.2008.07.009</td>\n",
       "      <td>Biol Psychiatry</td>\n",
       "      <td>2586327</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although our procedure was intended to target ...</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>10.1016/j.brainres.2008.07.008</td>\n",
       "      <td>Brain Res</td>\n",
       "      <td>2612637</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low status face pictures, and included genotyp...</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>10.1371/journal.pone.0004156</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>2612746</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For each word, participants made a recognition...</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>10.1016/j.neuron.2008.07.022</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>2614916</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  section  \\\n",
       "0  In the Golgi-impregnation experiment we found ...  DISCUSS   \n",
       "1  The individuals with at least 3 usable voxels ...  METHODS   \n",
       "2  Although our procedure was intended to target ...  METHODS   \n",
       "3  low status face pictures, and included genotyp...  METHODS   \n",
       "4  For each word, participants made a recognition...  METHODS   \n",
       "\n",
       "                              doi            Journal Title    pmcid  \\\n",
       "0       10.1007/s11481-008-9113-7  J Neuroimmune Pharmacol  2581635   \n",
       "1  10.1016/j.biopsych.2008.07.009          Biol Psychiatry  2586327   \n",
       "2  10.1016/j.brainres.2008.07.008                Brain Res  2612637   \n",
       "3    10.1371/journal.pone.0004156                 PLoS One  2612746   \n",
       "4    10.1016/j.neuron.2008.07.022                   Neuron  2614916   \n",
       "\n",
       "   data_statement  Year  \n",
       "0               0  2008  \n",
       "1               0  2008  \n",
       "2               0  2008  \n",
       "3               0  2009  \n",
       "4               0  2008  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nimh = pd.read_csv('/data/riddleta/data_sharing_reuse/external/nimh_papers.csv')\n",
    "df_nimh['Year'] = df_nimh['journal_year']\n",
    "df_nimh = df_nimh[['pmcid', 'Year']].drop_duplicates()\n",
    "df = df.merge(df_nimh, how='left', on='pmcid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_url'] = df.text.apply(lambda x: extract.has_urls(x))\n",
    "df['has_parenth'] = df.text.apply(lambda x: check_paren(x))\n",
    "df['repo'] = df.text.apply(lambda x: repo_label(x))\n",
    "df['text'] = df.text.apply(lambda x: sep_urls(x))\n",
    "df['syn_text'] = df.text.apply(lambda x: syns(x))\n",
    "df['all_text'] = df.text + ' ' + df.syn_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "x_tr, x_tst, y_tr, y_tst = train_test_split(df.all_text, df.data_statement, test_size=.33, random_state=42, stratify=df.data_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0    1\n",
      "True               \n",
      "0          804   75\n",
      "1            3  119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       879\n",
      "           1       0.61      0.98      0.75       122\n",
      "\n",
      "    accuracy                           0.92      1001\n",
      "   macro avg       0.80      0.95      0.85      1001\n",
      "weighted avg       0.95      0.92      0.93      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = cv.fit_transform(x_tr)\n",
    "one_hots_train = enc.fit_transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].loc[x_tr.index])\n",
    "y_train = df.data_statement[x_tr.index]\n",
    "x_test = cv.transform(df.all_text[x_tst.index])\n",
    "one_hots_test = enc.transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].iloc[x_tst.index])\n",
    "y_test = df.data_statement[x_tst.index]\n",
    "\n",
    "x_train = hstack([x_train, one_hots_train])\n",
    "x_test = hstack([x_test, one_hots_test])\n",
    "#x_res, y_res = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "clf = EasyEnsembleClassifier() #should have set random seed here\n",
    "#y_score = clf.fit(x_res, y_res).decision_function(x_test)\n",
    "y_score = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "y_pred_proba = clf.predict_proba(x_test)\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply code detector to hold-out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc[x_tst.index]\n",
    "df_test['kw_code'] = df_test.text.apply(lambda x: code_kw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['y_pred'] = y_pred\n",
    "df_test['y_prob'] = y_pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_rnd2 = df_test[df_test.y_pred==1]\n",
    "df_test_rnd2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_tst, y_tr, y_tst = train_test_split(df_test_rnd2.kw_code, df_test_rnd2.data_statement, test_size=.25, random_state=42, stratify=df_test_rnd2.data_statement)\n",
    "x_tr = df_test_rnd2[['kw_code', 'y_prob']].loc[x_tr.index]\n",
    "x_tst = df_test_rnd2[['kw_code', 'y_prob']].loc[x_tst.index]\n",
    "#x_tr.shape\n",
    "#x_tr = np.reshape(x_tr.tolist(), (x_tr.shape[0], 1))\n",
    "#x_tst = np.reshape(x_tst.tolist(), (x_tst.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log = LogisticRegression(random_state=42)\n",
    "clf.fit(x_tr, y_tr)\n",
    "y_pred = clf.predict(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0   1\n",
      "True             \n",
      "0          12   7\n",
      "1          10  20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.63      0.59        19\n",
      "           1       0.74      0.67      0.70        30\n",
      "\n",
      "    accuracy                           0.65        49\n",
      "   macro avg       0.64      0.65      0.64        49\n",
      "weighted avg       0.67      0.65      0.66        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(y_tst, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "print(classification_report(y_tst, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_sharing_reuse] *",
   "language": "python",
   "name": "conda-env-data_sharing_reuse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
