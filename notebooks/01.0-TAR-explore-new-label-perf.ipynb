{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, f1_score, precision_score, recall_score\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from urlextract import URLExtract\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.features.build_features import syns, sep_urls, check_paren, repo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = URLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_statement</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>Journal Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Page</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>PMID</th>\n",
       "      <th>cv_run0</th>\n",
       "      <th>cv_run1</th>\n",
       "      <th>cv_run2</th>\n",
       "      <th>cv_run3</th>\n",
       "      <th>cv_run4</th>\n",
       "      <th>cv_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1007/s11481-008-9113-7</td>\n",
       "      <td>2581635</td>\n",
       "      <td>DISCUSS</td>\n",
       "      <td>In the Golgi-impregnation experiment we found ...</td>\n",
       "      <td>J Neuroimmune Pharmacol</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>241</td>\n",
       "      <td>PMC2581635</td>\n",
       "      <td>18594991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1016/j.biopsych.2008.07.009</td>\n",
       "      <td>2586327</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>The individuals with at least 3 usable voxels ...</td>\n",
       "      <td>Biol Psychiatry</td>\n",
       "      <td>2008</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>856</td>\n",
       "      <td>PMC2586327</td>\n",
       "      <td>18707679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1016/j.brainres.2008.07.008</td>\n",
       "      <td>2612637</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Although our procedure was intended to target ...</td>\n",
       "      <td>Brain Res</td>\n",
       "      <td>2008</td>\n",
       "      <td>1230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202</td>\n",
       "      <td>PMC2612637</td>\n",
       "      <td>18662678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1371/journal.pone.0004156</td>\n",
       "      <td>2612746</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>low status face pictures, and included genotyp...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>e4156</td>\n",
       "      <td>PMC2612746</td>\n",
       "      <td>19142220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1016/j.neuron.2008.07.022</td>\n",
       "      <td>2614916</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>For each word, participants made a recognition...</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>2008</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>547</td>\n",
       "      <td>PMC2614916</td>\n",
       "      <td>18760691</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_statement                             doi    pmcid  section  \\\n",
       "0               0       10.1007/s11481-008-9113-7  2581635  DISCUSS   \n",
       "1               0  10.1016/j.biopsych.2008.07.009  2586327  METHODS   \n",
       "2               0  10.1016/j.brainres.2008.07.008  2612637  METHODS   \n",
       "3               0    10.1371/journal.pone.0004156  2612746  METHODS   \n",
       "4               0    10.1016/j.neuron.2008.07.022  2614916  METHODS   \n",
       "\n",
       "                                                text            Journal Title  \\\n",
       "0  In the Golgi-impregnation experiment we found ...  J Neuroimmune Pharmacol   \n",
       "1  The individuals with at least 3 usable voxels ...          Biol Psychiatry   \n",
       "2  Although our procedure was intended to target ...                Brain Res   \n",
       "3  low status face pictures, and included genotyp...                 PLoS One   \n",
       "4  For each word, participants made a recognition...                   Neuron   \n",
       "\n",
       "   Year Volume Issue   Page       PMCID      PMID  cv_run0  cv_run1  cv_run2  \\\n",
       "0  2008      3     4    241  PMC2581635  18594991        0        0        0   \n",
       "1  2008     64    10    856  PMC2586327  18707679        0        0        0   \n",
       "2  2008   1230   NaN    202  PMC2612637  18662678        0        0        0   \n",
       "3  2009      4     1  e4156  PMC2612746  19142220        0        0        0   \n",
       "4  2008     59     4    547  PMC2614916  18760691        0        0        0   \n",
       "\n",
       "   cv_run3  cv_run4  cv_sum  \n",
       "0        0        0       0  \n",
       "1        0        0       0  \n",
       "2        0        0       0  \n",
       "3        0        0       0  \n",
       "4        0        0       0  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/data/riddleta/data_sharing_reuse/external/combined_labels_incomplete.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.fillna('', inplace=True)\n",
    "df['has_url'] = df.text.apply(lambda x: extract.has_urls(x))\n",
    "df['has_parenth'] = df.text.apply(lambda x: check_paren(x))\n",
    "df['repo'] = df.text.apply(lambda x: repo_label(x))\n",
    "df['text'] = df.text.apply(lambda x: sep_urls(x))\n",
    "df['syn_text'] = df.text.apply(lambda x: syns(x))\n",
    "df['all_text'] = df.text + ' ' + df.syn_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_repo_mentioned      2367\n",
       "dbgap                    43\n",
       "github                   30\n",
       "fcon_1000.projects       18\n",
       "nitrc                    16\n",
       " ndar                    12\n",
       "loni.usc.edu             10\n",
       "osf.io                    9\n",
       "brain-map.org             9\n",
       "humanconnectome.org       9\n",
       "zenodo                    4\n",
       "figshare                  2\n",
       "dryad                     2\n",
       "fmridc                    1\n",
       "openneuro                 1\n",
       "Name: repo, dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repo.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_tst, y_tr, y_tst = train_test_split(df.all_text, df.data_statement, test_size=.25, random_state=42, stratify=df.data_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[572   9]\n",
      " [ 11  42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       581\n",
      "           1       0.82      0.79      0.81        53\n",
      "\n",
      "    accuracy                           0.97       634\n",
      "   macro avg       0.90      0.89      0.90       634\n",
      "weighted avg       0.97      0.97      0.97       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = cv.fit_transform(x_tr)\n",
    "one_hots_train = enc.fit_transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].loc[x_tr.index])\n",
    "y_train = df.data_statement[x_tr.index]\n",
    "x_test = cv.transform(df.all_text[x_tst.index])\n",
    "one_hots_test = enc.transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].iloc[x_tst.index])\n",
    "y_test = df.data_statement[x_tst.index]\n",
    "\n",
    "x_train = hstack([x_train, one_hots_train])\n",
    "x_test = hstack([x_test, one_hots_test])\n",
    "#x_res, y_res = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "#y_score = clf.fit(x_res, y_res).decision_function(x_test)\n",
    "y_score = clf.fit(x_train, y_train).decision_function(x_test)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Francisco meeting (1/24)\n",
    "\n",
    "Francisco suggested a few changes. First, look into adjustments to the classifier to deal with class imbalance. Second, in light of the reasonably good performance achieved by a single, small decision tree, it may be that a random forest classifier can yield mor eimprovements. Below implements those change\n",
    "\n",
    "### Adaboost w/ class imbalance adjustments\n",
    "\n",
    "I found an extension of scikit learn that specifically deals with imbalanced data. In their documentation, they describe the following classifier:\n",
    "\n",
    "The EasyEnsembleClassifier allows to bag AdaBoost learners which are trained on balanced bootstrap samples LWZ2009.\n",
    " \n",
    "with LWZ2009 referring to the following paper:\n",
    "\n",
    "X. Y. Liu, J. Wu and Z. H. Zhou, “Exploratory Undersampling for Class-Imbalance Learning,” in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 39, no. 2, pp. 539-550, April 2009.\n",
    "\n",
    "The below implements this modification. We see that we're now capturing nearly every data statement, though the precision has dropped a bit. I think for future labeling, this is a worthwhile trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0   1\n",
      "True              \n",
      "0          552  29\n",
      "1            2  51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       581\n",
      "           1       0.64      0.96      0.77        53\n",
      "\n",
      "    accuracy                           0.95       634\n",
      "   macro avg       0.82      0.96      0.87       634\n",
      "weighted avg       0.97      0.95      0.96       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = cv.fit_transform(x_tr)\n",
    "one_hots_train = enc.fit_transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].loc[x_tr.index])\n",
    "y_train = df.data_statement[x_tr.index]\n",
    "x_test = cv.transform(df.all_text[x_tst.index])\n",
    "one_hots_test = enc.transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].iloc[x_tst.index])\n",
    "y_test = df.data_statement[x_tst.index]\n",
    "\n",
    "x_train = hstack([x_train, one_hots_train])\n",
    "x_test = hstack([x_test, one_hots_test])\n",
    "#x_res, y_res = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "clf = EasyEnsembleClassifier()\n",
    "#y_score = clf.fit(x_res, y_res).decision_function(x_test)\n",
    "y_score = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Generally this doesn't appear to work as well as adaboost. Changing the class weights appears to have negligible impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0   1\n",
      "True              \n",
      "0          579   2\n",
      "1           16  37\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       581\n",
      "           1       0.95      0.70      0.80        53\n",
      "\n",
      "    accuracy                           0.97       634\n",
      "   macro avg       0.96      0.85      0.89       634\n",
      "weighted avg       0.97      0.97      0.97       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = cv.fit_transform(x_tr)\n",
    "one_hots_train = enc.fit_transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].loc[x_tr.index])\n",
    "y_train = df.data_statement[x_tr.index]\n",
    "x_test = cv.transform(df.all_text[x_tst.index])\n",
    "one_hots_test = enc.transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].iloc[x_tst.index])\n",
    "y_test = df.data_statement[x_tst.index]\n",
    "\n",
    "x_train = hstack([x_train, one_hots_train])\n",
    "x_test = hstack([x_test, one_hots_test])\n",
    "#x_res, y_res = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "#y_score = clf.fit(x_res, y_res).decision_function(x_test)\n",
    "y_score = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0   1\n",
      "True              \n",
      "0          574   7\n",
      "1           20  33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       581\n",
      "           1       0.82      0.62      0.71        53\n",
      "\n",
      "    accuracy                           0.96       634\n",
      "   macro avg       0.90      0.81      0.84       634\n",
      "weighted avg       0.95      0.96      0.95       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = cv.fit_transform(x_tr)\n",
    "one_hots_train = enc.fit_transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].loc[x_tr.index])\n",
    "y_train = df.data_statement[x_tr.index]\n",
    "x_test = cv.transform(df.all_text[x_tst.index])\n",
    "one_hots_test = enc.transform(df[['section', 'Journal Title', 'Year', 'has_url', 'has_parenth', 'repo']].iloc[x_tst.index])\n",
    "y_test = df.data_statement[x_tst.index]\n",
    "\n",
    "x_train = hstack([x_train, one_hots_train])\n",
    "x_test = hstack([x_test, one_hots_test])\n",
    "#x_res, y_res = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier(class_weight\n",
    "                            ) # adjusting this hasn't dramatically changed the predictions.\n",
    "#y_score = clf.fit(x_res, y_res).decision_function(x_test)\n",
    "y_score = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_sharing_reuse] *",
   "language": "python",
   "name": "conda-env-data_sharing_reuse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
